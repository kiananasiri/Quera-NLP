{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MeWq_Upjn7h"
      },
      "source": [
        "<div dir=center>\n",
        "  <h1 align=center style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
        "    <font face=\"vazir\" color=\"#0099cc\">\n",
        "      روژان و کلمه گمشده\n",
        "    </font>\n",
        "  </h1>\n",
        "</div>\n",
        "\n",
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" size=3>\n",
        "    در این تمرین، شما قدم اول برای استفاده از مدل‌های قدرتمند هوش مصنوعی در پردازش متن را تجربه خواهید کرد. داستان \"روژان و کلمه‌ی گمشده\"، منبع الهام شما برای یادگیری مهارت‌های پیش‌پردازش و تعبیه‌سازی داده‌های متنی خواهد بود. این مراحل، پایه و اساس تحلیل‌های پیشرفته مانند خوشه‌بندی و خلاصه‌سازی را فراهم می‌کنند. مدل‌های خوشه‌بندی و خلاصه‌سازی قبلاً برای شما آماده شده‌اند؛ وظیفه شما این است که داده‌های داستان را برای این مدل‌ها آماده کنید.\n",
        "  </font>\n",
        "</p>\n",
        "\n",
        "------\n",
        "\n",
        "<h1 dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" color=\"#0099cc\" size= 4>\n",
        "    اهداف تمرین:\n",
        "  </font>\n",
        "</h1>\n",
        "\n",
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" size=3>\n",
        "      در این تمرین ما قصد داریم با مفاهیمی که از اول تا به اینجا آشنا شده‌ایم کار کنیم. مفاهیمی از قبیل :<br>\n",
        "        <storng>آشنایی با پیش‌پردازش متن:</strong><br>\n",
        "        <div dir=\"rtl\">\n",
        "          <ul>\n",
        "            <li>یادگیری روش‌های استاندارد مانند توکن‌سازی، حذف کلمات توقف، و تبدیل متن به قالب عددی.</li>\n",
        "          </ul>\n",
        "          <strong>کار با تعبیه‌سازی کلمات:</strong><br>\n",
        "          <ul>\n",
        "            <li>استفاده از الگوریتم <code>Word2Vec</code> برای نمایش کلمات به صورت بردارهای عددی که معنای آن‌ها را در فضای چندبعدی نشان می‌دهد.</li>\n",
        "          </ul>\n",
        "          <strong>درک نقش پیش‌پردازش در مدل‌های پیشرفته:</strong><br>\n",
        "          <ul>\n",
        "            <li>مشاهده اهمیت کیفیت داده‌های ورودی در عملکرد مدل‌های هوش مصنوعی.</li>\n",
        "          </ul>\n",
        "        </div>\n",
        "  </font>\n",
        "</p>\n",
        "\n",
        "<h1 dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" color=\"#0099cc\" size=4>\n",
        "    مراحل تمرین:\n",
        "  </font>\n",
        "</h1>\n",
        "\n",
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" size=3>\n",
        "    همانطور که در اول تمرین بیان شد ما قصد داریم که یک داستان کوتاه را آماده پردازش و ارجاع به مدل‌های خوشه‌بندی و خلاصه‌سازی کنیم. در این تمرین داستان \"روژان و کلمه گمشده\" را باید پیش پردازش کرده و بروی آن تعبیه‌سازی را اعمال کنیم.<br>\n",
        "    به صورت کلی می‌توان انجام این تمرین را در سه مرحله زیر در نظر بگیریم.<br>\n",
        "        <storng>پیش‌پردازش متن:</strong><br>\n",
        "        <div dir=\"rtl\">\n",
        "          <ul>\n",
        "            <li><strong>نرمال‌سازی متن:</strong> تبدیل همه کلمات به حروف کوچک و حذف علائم نگارشی.</li>\n",
        "            <li><strong>توکن‌سازی داستان:</strong> تقسیم متن به کلمات یا جملات.</li>\n",
        "            <li><strong>حذف کلمات توقف:</strong> حذف کلمات پرتکرار و کم‌اهمیت (مانند <i>\"and\"</i>, <i>\"or\"</i>, ...).</li>\n",
        "            <li><strong>ریشه‌یابی کلمات <i>(lemmatization)</i>:</strong> برگرداندن کلمات به ریشه‌های اصلی (مانند <i>\"running\"</i> به <i>\"run\"</i>, ...).</li>\n",
        "          </ul>\n",
        "          <strong>تعبیه‌سازی متن <i>(Word Embedding)</i>:</strong><br>\n",
        "          <ul>\n",
        "            <li>استفاده از مدل <code>Word2Vec</code> برای تبدیل کلمات به بردارهای عددی.</li>\n",
        "            <li>نمایش بردارهای به دست آمده و بررسی شباهت بین کلمات مختلف (مثلاً شباهت بین \"شجاعت\" و \"عشق\").</li>\n",
        "          </ul>\n",
        "          <strong>آماده‌سازی برای تحلیل:</strong><br>\n",
        "          <ul>\n",
        "            <li>ذخیره داده‌های پیش‌پردازش شده و بردارهای تعبیه‌شده برای استفاده در مدل‌های خوشه‌بندی و خلاصه‌سازی.</li>\n",
        "          </ul>\n",
        "        </div>\n",
        "  </font>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gaivn8cdqjeM"
      },
      "source": [
        "<h1 dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" color=\"#0099cc\" size=5>\n",
        "    گام اول : فراخوانی کتابخانه‌ها\n",
        "  </font>\n",
        "</h1>\n",
        "\n",
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" size=3>\n",
        "    در مرحله اول نیاز داریم تا کتابخانه‌هایی که مورد نیاز هستند در این برنامه را فراخوانی کنیم.\n",
        "  </font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rMlJtwDSzI9"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb1wIDCGqjeO"
      },
      "source": [
        "<h1 dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" color=\"#0099cc\" size=5>\n",
        "    گام دوم : بازکردن فایل داستان\n",
        "  </font>\n",
        "</h1>\n",
        "\n",
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" size=3>\n",
        "    در این مرحله شما می‌بایست که فایل <code>Rojan and Lost Word.txt</code> ذخیره شده در مسیر <code>data\\Rojan and Lost Word.txt</code> را باز کرده و محتویات آن را بخوانید و در متغیری به نام <code>texts</code> ذخیره کنید.<br>\n",
        "    بهتر است محتویات خوانده شده را براساس <storng> کاراکتر <code>\"n\\\"</code> </storng> تیکه کنید. تا بهتر بتوان جملات یک خط را در نظر گرفت. <br>\n",
        "    با انجام عملیات بالا محتویات متغیر <code>texts</code> به شکل زیر خواهد بود:\n",
        "  </font>\n",
        "</p>\n",
        "\n",
        "  ['Rojan and the Lost Word',<br>\n",
        " 'In a land where words held magic and every phrase was a bridge to an extraordinary concept, there lived a curious and imaginative girl named Rojan. She adored words—words that could heal wounds, bring joy to hearts, or unveil the profound secrets of the universe.',<br>\n",
        " 'One day, in the small village marketplace, an old man approached her. His robe seemed to be woven from the pages of ancient books, and his eyes sparkled like two stars in the night. In a gentle voice, he said:',\n",
        " '“Child, did you know that in every person’s heart, there is a lost word? A word that, once found, will reveal the meaning of life.”',\n",
        " 'Amazed, Rojan asked, “What is that word?”', ...]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHpUXjzRTKHS"
      },
      "outputs": [],
      "source": [
        "#TO Do\n",
        "\n",
        "# read the \"Rojan and Lost Word.txt\" from the \"data\" directory and save it in \"text\" variable\n",
        "file  = # to_do\n",
        "texts = # to_do\n",
        "texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hK2ruV_gqjeQ"
      },
      "source": [
        "<h1 dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" color=\"#0099cc\" size=5>\n",
        "    گام سوم : انجام مراحل پیش‌پردازش متن\n",
        "  </font>\n",
        "</h1>\n",
        "\n",
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" size=3>\n",
        "    در این مرحله می‌بایست تابعی را تعریف کنیم که عملیات‌های زیر را برروی متن‌های ورودی اعمال کرده و یک لیست را به عنوان خروجی برگرداند.<br>\n",
        "    نام این تابع <code>preprocessing_text</code> است که به عنوان پارامتر ورودی یک پارامتر رشته‌ای که در واقع جملات داستان ما هست را به نام <code>text</code> دریافت می‌کند.<br>\n",
        "    <div dir=\"rtl\">\n",
        "      <ul>\n",
        "        <li><strong>مرحله اول:</strong> تمام حروف باید تبدیل به حروف کوچک شوند.</li>\n",
        "        <li><strong>مرحله دوم:</strong>متن ورودی باید به صورت کلمه‌ای توکن‌سازی شود.</li>\n",
        "        <li><strong>مرحله سوم:</strong>تمام کاراکترهای غیر از حروف و فاصله‌ها را  از داخل عبارات پاک می‌کنیم مانند اعداد کاراکتر‌ها و شکلک‌ها و ... .</li>\n",
        "        <li><strong>مرحله چهارم:</strong>علائم نگارشی و کلمات توقف از لیست پاک شوند <i>(stopwords)</i>.</li>\n",
        "        <li><strong>مرحله پنجم:</strong>تنها توکن‌هایی را در نظر بگیرید که رشته خالی نباشند.</li>\n",
        "        <li><strong>مرحله ششم:</strong> ریشه‌یابی تمام توکن‌ها با استفاده از <i>(Lemmatization)</i></li>\n",
        "        <li><strong>مرحله هفتم:</strong> لیست نهایی که شامل توکن‌هایست که با استفاده از روش‌های بالا پیش‌پردازش شده است را به عنوان خروجی به برنامه بر‌می‌گردانیم</li>\n",
        "      </ul>\n",
        "      به عنوان مثال پس از اعمال تابع پیش‌پردازش روی جمله اول یعنی <i>Rojan and the Lost Word</i> خروجی‌ای مشابه زیر خواهیم داشت:<br>\n",
        "    </div>\n",
        "  </font>\n",
        "</p>\n",
        "\n",
        "<code>['rojan', 'lost', 'word']</code>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISkwBq1OTsOo"
      },
      "outputs": [],
      "source": [
        "# TO DO\n",
        "\n",
        "# import the WordNetLemmatizer from nltk.stem for lemmatization\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# download the important packages for preprocessing\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "# create a punctuation set for preprocessing\n",
        "punctuation  = string.punctuation\n",
        "punctuation  += \"`\\\"\"\n",
        "\n",
        "# define the \"preprocessing_text\" function\n",
        "def preprocessing_text(text:str) -> list:\n",
        "    # to_do\n",
        "    return texts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu8aUqIeqjeR"
      },
      "source": [
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" size=3>\n",
        "    بعد از تعریف تابع مورد نظر قسمت بالا شروع به پیش‌پردازش هر متن موجود در متغیر <code>texts</code> می‌کنیم و نتیجه این عملیات را در متغیری به نام <code>text_tokens</code> به شکل یک لیست ذخیره میکنیم.<br><br>\n",
        "    اگر 2 داده‌ی اول ذخیره شده در لیست را مشاهده کنیم، خروجی به شکل زیر خواهیم یافت:\n",
        "  </font>\n",
        "</p>\n",
        "\n",
        "<pre>\n",
        "<code>\n",
        "print( text_tokens[:2] )\n",
        "<code>\n",
        "[['rojan', 'lost', 'word'],\n",
        "['land', 'word', 'held', 'magic', 'every', 'phrase', 'bridge', 'extraordinary', 'concept', 'lived', 'curious', 'imaginative', 'girl', 'named', 'rojan',  'adored', 'wordswords', 'could', 'heal', 'wound', 'bring', 'joy', 'heart', 'unveil', 'profound', 'secret', 'universe']]\n",
        "<pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f2s38OYWd40"
      },
      "outputs": [],
      "source": [
        "# TO DO\n",
        "\n",
        "# preprocessing the 'texts' into the 'texts_tokens'\n",
        "texts_tokens = # to_do\n",
        "texts_tokens[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUAsh9HdDpyi"
      },
      "source": [
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" size=3>\n",
        "    برای کلمات پرتکرار در متن نمودار <i><strong>wordcloud</strong></i> را ایجاد می‌کنیم.\n",
        "  </font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7rhpXA---DN"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "frequency = Counter([word for sentence in texts_tokens for word in sentence])\n",
        "\n",
        "# Convert the Counter object to a string suitable for WordCloud\n",
        "text = \" \".join(f\"{word} \" * count for word, count in frequency.items())\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color=\"white\", colormap=\"viridis\").generate(text)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud of Text\", fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zcg34FY4qjeS"
      },
      "source": [
        "<h1 dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" color=\"#0099cc\" size=5>\n",
        "    گام چهارم : آموزش مدل‌ <i>Word2Vec</i>\n",
        "  </font>\n",
        "</h1>\n",
        "\n",
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" size=3>\n",
        "    حال که تمام داده‌های ما پیش‌پردازش‌های لازم را دیده‌اند و به حالت پایه و بدون نویز رسیدن نوبت به تعبیه با استفاده از الگوریتم <i>Word2Vec</i> می‌رسد.<br>\n",
        "    <br>\n",
        "    برای آموزش این مدل می‌توان از کتابخانه <code>Gensim</code> و ماژول <code>models</code> استفاده کنیم.\n",
        "  </font>\n",
        "</p>\n",
        "<pre>\n",
        "<code>\n",
        "Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "</code>\n",
        "</pre>\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "<table class=\"center\">\n",
        "  <tr>\n",
        "    <th>پارامتر</th>\n",
        "    <th>کاربرد</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>sentences</td>\n",
        "    <td>مجموعه‌ای که تعبیه متن باید روی آن انجام شود</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>vector_size</td>\n",
        "    <td>اندازه بردار‌های تعبیه (در واقع تعداد ویژگی‌های هرکلمه)</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>window</td>\n",
        "    <td>اندازه پنچره‌ای که در عملیات‌های <i>skip-gram</i> یا <i>cbow</i> مورد استفاده قرار می‌گیرد </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>min_count</td>\n",
        "    <td>مشخص می‌کند هر کلمه حداقل باید چندبار در متن مشاهده شود تا در مدل‌سازی مورد استفاده قرار بگیرد</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>workers</td>\n",
        "    <td>تعداد هسته‌هایی که برای آموزش در نظر گرفته می‌شود </td>\n",
        "  </tr>\n",
        "</table>\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2d9cjP9Wjcc"
      },
      "outputs": [],
      "source": [
        "# TO DO\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train the Word2Vec model\n",
        "model = # to_do"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U0ftOutqjeT"
      },
      "source": [
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" size=3>\n",
        "        حال که مدل خود را آموزش دادیم قصد داریم یک دیتافریم <i>(dataframe)</i> برای شبیه ترین کلمات به کلمه <i>love</i> را ایجاد کنیم.\n",
        "        <br>\n",
        "        ۱. در گام اول با استفاده از <code>model</code> آموزش دیده شبیه‌ترین کلمات به کلمه <i>love</i> را پیدا کرده سپس در متغیری بنام <code>love_words</code> ذخیره کنید.<br>\n",
        "        ۲. حال با استفاده از نتیجه به دست آمده یک دیتافریم ایجاد کرده و در متغیر <code>love_words_df</code> ذخیره کنید.\n",
        "  </font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvPxuj_DqjeT"
      },
      "source": [
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" size=3>\n",
        "    جدول دیتافریم شما باید جدولی مشابه زیر باشد.\n",
        "  </font>\n",
        "</p>\n",
        "\n",
        "<table class=\"center\">\n",
        "  <tr>\n",
        "    <th>love_words</th>\n",
        "    <th>love_pro</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>realized</td>\n",
        "    <td>0.29647910594940186</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>first</td>\n",
        "    <td>0.3072448670864105</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>gave</td>\n",
        "    <td>0.21588125824928284</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>...</td>\n",
        "    <td>...</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" size=3>\n",
        "    جدول نمایش داده شده در واقع یک جدول نمونه جهت فهم مخاطب است  و جدول مخاطب دارای ۱۰ ردیف می‌باشد و با جدول نمونه متفاوت خواهد بود\n",
        "  </font>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHxpEp6vXYOW"
      },
      "outputs": [],
      "source": [
        "# TO DO\n",
        "\n",
        "love_words = # to_do\n",
        "love_words_df = # to do\n",
        "love_words_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ozg8Ym-iArwa"
      },
      "source": [
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" size=3>\n",
        "    با استفاده از کد زیر می‌توانیم یک نمودار <strong>histogram</strong> برای احتمال شباهت هر کلمه با کلمه <i>love</i> رو مشخص کنیم.\n",
        "  </font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVPy3UXOAR_K"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Create the histogram plot\n",
        "fig = px.histogram(love_words_df, x=\"love_words\", y=\"love_pro\")\n",
        "\n",
        "# Customize the plot (optional)\n",
        "fig.update_layout(\n",
        "    title=\"Histogram of Love Probabilities per Word\",\n",
        "    xaxis_title=\"Words\",\n",
        "    yaxis_title=\"Love Probability\"\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUtIpdAOqjeU"
      },
      "source": [
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" size=3>\n",
        "        حال باید یک دیتافریم <i>(dataframe)</i> برای شبیه ترین کلمات به کلمه <i>friendship</i> را ایجاد کنیم.\n",
        "        <br>\n",
        "        ۱. در گام اول با استفاده از <code>model</code> آموزش دیده شبیه‌ترین کلمات به کلمه <i>friendship</i> را پیدا کرده سپس در متغیری بنام <code>friendship_words</code> ذخیره کنید.<br>\n",
        "        ۲. حال با استفاده از نتیجه به دست آمده یک دیتافریم ایجاد کرده و در متغیر <code>friendship_words_df</code> ذخیره کنید.\n",
        "  </font>\n",
        "</p>\n",
        "\n",
        "<table class=\"center\">\n",
        "  <tr>\n",
        "    <th>friendship_words</th>\n",
        "    <th>friendship_pro</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>realized</td>\n",
        "    <td>0.29647910594940186</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>first</td>\n",
        "    <td>0.3072448670864105</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>gave</td>\n",
        "    <td>0.21588125824928284</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>...</td>\n",
        "    <td>...</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" size=3>\n",
        "    جدول نمایش داده شده در واقع یک جدول نمونه جهت فهم مخاطب است  و جدول مخاطب دارای ۱۰ ردیف می‌باشد و با جدول نمونه متفاوت خواهد بود\n",
        "  </font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJ-WDLEoXfbJ"
      },
      "outputs": [],
      "source": [
        "# TO DO\n",
        "friendship_words =  # to do\n",
        "friendship_words_df = # to do\n",
        "friendship_words_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl68v18mCmua"
      },
      "source": [
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" size=3>\n",
        "    با استفاده از کد زیر می‌توانیم یک نمودار <strong>histogram</strong> برای احتمال شباهت هر کلمه با کلمه <i>friendship</i> رو مشخص کنیم.\n",
        "  </font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRjLosW4CvzW"
      },
      "outputs": [],
      "source": [
        "# Create the histogram plot\n",
        "fig = px.histogram(friendship_words_df, x=\"friendship_words\", y=\"friendship_pro\")\n",
        "\n",
        "# Customize the plot (optional)\n",
        "fig.update_layout(\n",
        "    title=\"Histogram of Friendship Probabilities per Word\",\n",
        "    xaxis_title=\"Words\",\n",
        "    yaxis_title=\"friendship Probability\"\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD3tb1CbqjeV"
      },
      "source": [
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" size=3>\n",
        "         یک دیتافریم <i>(dataframe)</i> برای شبیه ترین کلمات به کلمه <i>courage</i> را ایجاد کنیم.\n",
        "        <br>\n",
        "        ۱. در گام اول با استفاده از <code>model</code> آموزش دیده شبیه‌ترین کلمات به کلمه <i>courage</i> را پیدا کرده سپس در متغیری بنام <code>courage_words</code> ذخیره کنید.<br>\n",
        "        ۲. حال با استفاده از نتیجه به دست آمده یک دیتافریم ایجاد کرده و در متغیر <code>courage_words_df</code> ذخیره کنید.\n",
        "  </font>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "<table class=\"center\">\n",
        "  <tr>\n",
        "    <th>courage_words</th>\n",
        "    <th>courage_pro</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>realized</td>\n",
        "    <td>0.29647910594940186</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>first</td>\n",
        "    <td>0.3072448670864105</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>gave</td>\n",
        "    <td>0.21588125824928284</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>...</td>\n",
        "    <td>...</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" size=3>\n",
        "    جدول نمایش داده شده در واقع یک جدول نمونه جهت فهم مخاطب است  و جدول مخاطب دارای ۱۰ ردیف می‌باشد و با جدول نمونه متفاوت خواهد بود\n",
        "  </font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcQDYE7AXpE4"
      },
      "outputs": [],
      "source": [
        "courage_words = # to do\n",
        "courage_words_df = # to do\n",
        "courage_words_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqvePcotDTzF"
      },
      "source": [
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "  <font face=\"vazir\" size=3>\n",
        "    با استفاده از کد زیر می‌توانیم یک نمودار <strong>histogram</strong> برای احتمال شباهت هر کلمه با کلمه <i>courage</i> رو مشخص کنیم.\n",
        "  </font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJeVC8c0DTzK"
      },
      "outputs": [],
      "source": [
        "# Create the histogram plot\n",
        "fig = px.histogram(courage_words_df, x=\"courage_words\", y=\"courage_pro\")\n",
        "\n",
        "# Customize the plot (optional)\n",
        "fig.update_layout(\n",
        "    title=\"Histogram of Courage Probabilities per Word\",\n",
        "    xaxis_title=\"Words\",\n",
        "    yaxis_title=\"Courage Probability\"\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeR_I8LXqjeV"
      },
      "source": [
        "<h1 dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "    <font face=\"vazir\" color=\"#0099cc\" size=5>\n",
        "        گام پنجم : انجام خوشه‌بندی بر روی جملات\n",
        "    </font>\n",
        "</h1>\n",
        "\n",
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "    <font face=\"vazir\" size=3>\n",
        "        در این مرحله، ما از الگوریتم خوشه‌بندی <i>K-Means</i> برای گروه‌بندی جملات استفاده خواهیم کرد. برای این کار، ابتدا باید تعبیه‌سازی جملات را با استفاده از میانگین بردارهای کلمات انجام دهیم. سپس، بر اساس این تعبیه‌ها، خوشه‌ها را تشکیل خواهیم داد. این مراحل به شما کمک می‌کند تا الگوهای موجود در متن را شناسایی کنید.\n",
        "    </font>\n",
        "</p>\n",
        "\n",
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "    <font face=\"vazir\" size=3>\n",
        "        به عنوان مثال، فرض کنید که جملات زیر را داریم:\n",
        "    </font>\n",
        "</p>\n",
        "\n",
        "<code>\n",
        "texts_tokens = [<br>\n",
        "['rojan', 'lost', 'word'],<br>\n",
        "['in', 'land', 'word', 'held', 'magic'],<br>\n",
        "['child', 'know', 'person', 'heart', 'lost', 'word'],<br>\n",
        "['amazed', 'rojan', 'asked', 'what', 'word']<br>\n",
        "]<br>\n",
        "</code>\n",
        "\n",
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "    <font face=\"vazir\" size=3>\n",
        "        پس از محاسبه میانگین بردارهای کلمات برای هر جمله، به عنوان مثال، می‌توانیم خوشه‌بندی را با استفاده از KMeans انجام دهیم:\n",
        "    </font>\n",
        "</p>\n",
        "\n",
        "<pre>\n",
        "<code>\n",
        "from sklearn.cluster import KMeans\n",
        "# تعداد خوشه‌ها را تعیین کنید\n",
        "num_clusters = 2\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "kmeans.fit(sentence_embeddings)\n",
        "# نمایش خوشه‌ها\n",
        "for i, label in enumerate(kmeans.labels_):\n",
        "    print(f\"جمله {i+1} به خوشه {label} تعلق دارد.\")\n",
        "</code>\n",
        "</pre>\n",
        "\n",
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "    <font face=\"vazir\" size=3>\n",
        "        نتایج ممکن است به شکل زیر باشد:\n",
        "    </font>\n",
        "</p>\n",
        "\n",
        "<pre>\n",
        "<code>\n",
        "# خروجی نمونه\n",
        "# جمله 1 به خوشه 0 تعلق دارد.\n",
        "# جمله 2 به خوشه 1 تعلق دارد.\n",
        "# جمله 3 به خوشه 0 تعلق دارد.\n",
        "# جمله 4 به خوشه 1 تعلق دارد.\n",
        "</code>\n",
        "</pre>\n",
        "\n",
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "    <font face=\"vazir\" size=3>\n",
        "        با این کار، شما می‌توانید جملات مشابه را در یک خوشه گروه‌بندی کنید و تحلیل‌های بهتری بر روی داده‌ها انجام دهید.\n",
        "    </font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxhw_EYDX1t0"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Generate sentence embeddings (average of word vectors)\n",
        "sentence_embeddings = []\n",
        "for sentence in texts_tokens:\n",
        "  if sentence:  # Check if the sentence is not empty\n",
        "    embeddings = [model.wv[word] for word in sentence if word in model.wv]\n",
        "    if embeddings: # Check if embeddings are found\n",
        "      sentence_embeddings.append(np.mean(embeddings, axis=0))\n",
        "\n",
        "# Apply KMeans clustering (example using 3 clusters)\n",
        "kmeans = KMeans(n_clusters=3, random_state=0)\n",
        "if sentence_embeddings:\n",
        "  kmeans.fit(sentence_embeddings)\n",
        "  cluster_labels = kmeans.labels_\n",
        "\n",
        "  # Now you have cluster_labels for each sentence in your data\n",
        "  for i, label in enumerate(cluster_labels):\n",
        "    print(f\"Sentence {i+1} belongs to cluster {label}\")\n",
        "else:\n",
        "    print(\"No valid sentence embeddings found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx8gNqkRqjeW"
      },
      "source": [
        "<h1 dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "    <font face=\"vazir\" color=\"#0099cc\" size=5>\n",
        "        گام ششم: انجام مراحل خلاصه‌سازی\n",
        "    </font>\n",
        "</h1>\n",
        "\n",
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "    <font face=\"vazir\" size=3>\n",
        "        در این مرحله می‌بایست یک تابع تعریف کنیم که متن‌های ورودی را به صورت خلاصه شده برگرداند. این تابع می‌تواند از الگوریتم‌های مختلفی برای خلاصه‌سازی استفاده کند، مانند <strong>خلاصه‌سازی استخراجی</strong> یا <strong>خلاصه‌سازی انتزاعی</strong>.<br>\n",
        "        به عنوان مثال، می‌توانیم از الگوریتم <code>Cluster-based Text Summarization</code> برای انتخاب جملات کلیدی استفاده کنیم. این الگوریتم بر اساس شباهت جملات، جملات مهم را استخراج می‌کند.<br>\n",
        "        <strong>مثال:</strong> فرض کنید که ما می‌خواهیم متن زیر را خلاصه کنیم:\n",
        "    </font>\n",
        "</p>\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "\"روژان دختری کنجکاو و خیال‌پرداز است که عاشق کلمات است. او در یک روز عادی با یک مرد سالخورده در بازار روستا ملاقات می‌کند. مرد به او می‌گوید که هر شخص یک کلمه گمشده دارد که با پیدا کردن آن، معنی زندگی برای او آشکار می‌شود.\"\n",
        "</div>\n",
        "\n",
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "    <font face=\"vazir\" size=3>\n",
        "        پس از اعمال تابع خلاصه‌سازی، خروجی ممکن است به شکل زیر باشد:\n",
        "    </font>\n",
        "</p>\n",
        "\n",
        "<div dir=\"rtl\">\n",
        "    \"روژان دختری کنجکاو است که با مردی سالخورده ملاقات می‌کند. او می‌گوید که هر شخص یک کلمه گمشده دارد.\"\n",
        "</div>\n",
        "\n",
        "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "    <font face=\"vazir\" size=3>\n",
        "        با استفاده از این روش‌ها، می‌توانید جملات کلیدی را استخراج کرده و متن را به صورت خلاصه ارائه دهید.\n",
        "    </font>\n",
        "</p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_yo4TybYDQ2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to summarize text based on cluster centroids\n",
        "def summarize_text(texts_tokens, cluster_labels, model, num_sentences=3):\n",
        "    # Check if there are any sentence embeddings; if not, return a message\n",
        "    if not sentence_embeddings:\n",
        "        return \"No sentences to summarize.\"\n",
        "\n",
        "    # Initialize a list to store the centroids of each cluster\n",
        "    cluster_centroids = []\n",
        "\n",
        "    # Iterate over all clusters and calculate their centroids\n",
        "    for i in range(kmeans.n_clusters):\n",
        "        # Get the indices of sentences belonging to the current cluster\n",
        "        cluster_indices = [j for j, label in enumerate(cluster_labels) if label == i]\n",
        "        if cluster_indices:\n",
        "            # Collect embeddings of sentences in the current cluster\n",
        "            cluster_embeddings = [sentence_embeddings[j] for j in cluster_indices]\n",
        "            # Calculate the centroid of the current cluster and append to the list\n",
        "            cluster_centroids.append(np.mean(cluster_embeddings, axis=0))\n",
        "\n",
        "    # Initialize the summary string\n",
        "    summary = \"\"\n",
        "\n",
        "    # Iterate over each cluster centroid to create a summary\n",
        "    for i, centroid in enumerate(cluster_centroids):\n",
        "        # Calculate similarities between the centroid and all sentence embeddings\n",
        "        similarities = []\n",
        "        for j, embedding in enumerate(sentence_embeddings):\n",
        "            # Compute cosine similarity between the centroid and the sentence embedding\n",
        "            similarity = np.dot(centroid, embedding) / (np.linalg.norm(centroid) * np.linalg.norm(embedding))\n",
        "            similarities.append((similarity, j))\n",
        "\n",
        "        # Sort the sentences by similarity in descending order\n",
        "        similarities.sort(reverse=True)\n",
        "\n",
        "        # Append the cluster's summary to the overall summary\n",
        "        summary += f\"Cluster {i+1} summary:\\n\"\n",
        "        for _, index in similarities[:num_sentences]:\n",
        "            # Retrieve the original sentence from the texts\n",
        "            original_sentence = texts[index]\n",
        "            summary += f\"- {original_sentence}\\n\"\n",
        "        summary += \"\\n\"\n",
        "\n",
        "    # Return the generated summary\n",
        "    return summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0n5qmqdkbM5"
      },
      "outputs": [],
      "source": [
        "summary = summarize_text(texts_tokens, cluster_labels, model)\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX_W7gqcqvDv"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "  <p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
        "    <font face=\"vazir\" size=3>\n",
        "      در این مرحله ما قصد داریم شباهت‌هایی که مدل برای کلمات <i>love</i> ،<i>friendship</i> و <i>courage</i> پیدا کرد رو به عنوان یک دیتافریم به عنوان جواب تمرین در متغیر <code>submission</code> ذخیره کنیم.\n",
        "    </font>\n",
        "  </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBH0aOf0si1b"
      },
      "outputs": [],
      "source": [
        "submission = pd.merge(love_words_df, friendship_words_df, left_index=True, right_index=True)\n",
        "submission = pd.merge(submission, courage_words_df, left_index=True, right_index=True)\n",
        "submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehBMQSo4r7tq"
      },
      "source": [
        "<h2 align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
        "<font face=\"vazir\" color=\"#0099cc\">\n",
        "<b>سلول جواب‌ساز</b>\n",
        "</font>\n",
        "</h2>\n",
        "\n",
        "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
        "<font face=\"vazir\" size=3>\n",
        "    برای ساخته‌شدن فایل <code>result.zip</code> سلول زیر را اجرا کنید. توجه داشته باشید که پیش از اجرای سلول زیر تغییرات اعمال شده در نت‌بوک را ذخیره کرده باشید (<code>ctrl+s</code>) تا در صورت نیاز به پشتیبانی امکان بررسی کد شما وجود داشته باشد.\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8i5hAOi7r8Zx"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import joblib\n",
        "\n",
        "def compress(file_names):\n",
        "    print(\"File Paths:\")\n",
        "    print(file_names)\n",
        "    compression = zipfile.ZIP_DEFLATED\n",
        "    with zipfile.ZipFile(\"result.zip\", mode=\"w\") as zf:\n",
        "        for file_name in file_names:\n",
        "            zf.write('./' + file_name, file_name, compress_type=compression)\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "file_names = [ 'submission.csv']\n",
        "compress(file_names)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
